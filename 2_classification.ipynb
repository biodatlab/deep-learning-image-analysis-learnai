{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Convolutional Neural Networks (CNN)**\n",
    "ใน Notebook นี้เราจะมาลงมือทดลองสร้างและเทรน CNN เบื้องต้นด้วยไลบรารี่ Pytorch, Pytorch Lightning, และ FastAI รวมถึงการทำ Transfer Learning กัน\n",
    "\n",
    "\n",
    "<img src=\"images/02_cnn_guide.png\" width=\"600\"/>\n",
    "\n",
    "Convolutional Neural Network (CNN) ประกอบด้วย Convolutional layer, Pooling layer (ได้หลายขั้น) ตามด้วย Fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available() # ตรวจสอบว่ามี CUDA หรือ GPU ที่สามารถใช้ได้หรือไม่\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# อ้างอิง: https://www.kaggle.com/code/pankajj/fashion-mnist-with-pytorch-93-accuracy/notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", download=True, transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"./data\", download=True, train=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionCNN(nn.Module):\n",
    "    \"\"\"Convolutional Neural Network เพื่อ Classify FashionMNIST\"\"\"\n",
    "    def __init__(self):\n",
    "        super(FashionCNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # บางครั้งเราเขียนแค่ nn.Conv2d(1, 32, 3, 1) \n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(in_features=64 * 7 * 7, out_features=600) # รู้ได้อย่างไรว่าเป็น 64*7*7 เราจะไปดูกันใน cell ถัดไป\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = torch.flatten(out, 1) # อาจจะใช้ view ก็ได้\n",
    "        out = self.fc1(out)\n",
    "        out = F.ReLU(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.ReLU(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### มาดู Dimension ของรูปภาพหลังผ่าน layer ต่างๆกัน\n",
    "\n",
    "Dimension หลังจากผ่าน Convolutional layer สามารถคำนวณได้ด้วย (W - K - 2P)/S - 1 โดย\n",
    "- W คือความกว้างหรือความยาวของรูปก่อนเข้า Convolutional layer\n",
    "- K คือ Kernel size\n",
    "- P คือ Padding จากภาพ\n",
    "- S คือ Stride (ส่วนมากจะใช้ค่าเท่ากับ 1)\n",
    "\n",
    "\n",
    "ส่วน MaxPool2d layer ทำการดึงข้อมูลที่มีค่าสูงที่สุดจาก stride ที่เรากำหนด โดยพื้นฐานส่วนมากเราจะใช้ kernel size = 2, stride = 2\n",
    "\n",
    "มี Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_img = torch.rand(1,28,28)\n",
    "print(f\"Image size: {dummy_img.size()}\")\n",
    "\n",
    "# ทดลองสร้าง Convolutional layer ที่รับ input depth = 1, output = 32, kernel size = 3, padding = 1\n",
    "conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "maxpool1= nn.MaxPool2d(kernel_size=(2,2))\n",
    "out1 = conv1(dummy_img)\n",
    "print(f\"Output of conv1: {out1.size()}\") # (32, 28, 28)\n",
    "out1 = maxpool1(out1)\n",
    "print(f\"Output of maxpool1: {out1.size()}\") # (32, 14, 14)\n",
    "\n",
    "conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "maxpool2= nn.MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "out2 = conv2(out1)\n",
    "print(f\"Output of conv2: {out2.size()}\") # (64, 14, 14)\n",
    "out2 = maxpool2(out2)\n",
    "print(f\"Output of maxpool2: {out2.size()}\") # (64, 7, 7)\n",
    "\n",
    "# สังเกต dimension ของ output นี้ว่ามิติแรกเท่ากับจำนวน image filters ของ conv2\n",
    "# และ มิติที่สองและสามมีค่าเท่ากับ (28, 28) หารด้วย 4 ทั้งสองมิติ หรือ (7,7) \n",
    "# เนื่องจากผ่านการ maxpool ด้วย kernel_size = (2,2) ถึง 2 ครั้งนั่นเอง"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ไม่มี Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_img = torch.rand(1,28,28)\n",
    "print(f\"Image size: {dummy_img.size()}\")\n",
    "\n",
    "# ทดลองสร้าง Convolutional layer ที่รับ input depth = 1, output = 32, kernel size = 3\n",
    "conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "maxpool1= nn.MaxPool2d(kernel_size=(2,2))\n",
    "out1 = conv1(dummy_img)\n",
    "print(f\"Output of conv1: {out1.size()}\") # (32, 26, 26)\n",
    "out1 = maxpool1(out1)\n",
    "print(f\"Output of maxpool1: {out1.size()}\") # (32, 13, 13)\n",
    "\n",
    "conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "maxpool2= nn.MaxPool2d(kernel_size=(2,2))\n",
    "\n",
    "out2 = conv2(out1)\n",
    "print(f\"Output of conv2: {out2.size()}\") # (64, 11, 11)\n",
    "out2 = maxpool2(out2)\n",
    "print(f\"Output of maxpool2: {out2.size()}\") # (64, 5, 5)\n",
    "\n",
    "# สังเกต dimension ของ output นี้ว่ามิติแรกเท่ากับจำนวน image filters ของ conv2\n",
    "# และ มิติที่สองและสามมีค่าเท่ากับ (28, 28) หารด้วย 4 แล้วลบด้วย 2 ทั้งสองมิติ หรือ (5,5)\n",
    "# เนื่องจากผ่านการ maxpool ด้วย kernel_size = (2,2) ถึง 2 ครั้ง (การหาร 4) \n",
    "# และลบด้วย 2 เนื่องจากเป็นส่วนขอบรูปที่ตกไปในการ filter แบบไม่เติม padding นั่นเอง"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train our FashionCNN**\n",
    "\n",
    "เทรนโมเดล CNN โดยใช้ Loop การเรียนรู้ที่เราสร้างขึ้น จากนั้นเราทำการทดสอบโมเดล"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, n_batch = 100, device = \"cuda\"):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Iterate over the dataloader\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Report loss every 100 batch\n",
    "        if batch % n_batch == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, device = \"cuda\"):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Prevent Pytorch from updating the gradients at the testing steps!\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "net = FashionCNN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss() # loss function for classification\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate) # Optimizer\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"min\")\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, net, loss_fn, optimizer, device = device)\n",
    "    val_loss = test_loop(test_dataloader, net, loss_fn, device = device)\n",
    "    scheduler.step(val_loss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DVEGT0chL8V"
   },
   "source": [
    "## **Transfer Learning ด้วย Pytorch**\n",
    "\n",
    "- Transfer learning เป็นหนึ่งในเทคนิคของ Machine learning ที่นำโมเดลที่สร้างมาสำหรับงานประเภทหนึ่งถูกนำมาใช้กับงานอีกประเภทหนึ่ง\n",
    "- หลักการโดยคร่าวๆของ Transfer learning คือการนำโมเดลที่ถูกเทรนมาแล้ว (Pre-trained model) มาทำการดัดแปลงพารามิเตอร์บางส่วนจากโมเดลเดิมและเทรนโมเดลสำหรับข้อมูลใหม่ของเรา\n",
    "\n",
    "\n",
    "อ้างอิง: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html#convnet-as-fixed-feature-extractor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "class TransferNet(nn.Module):\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super(TransferNet, self).__init__()\n",
    "        self.model = backbone\n",
    "        # \"freeze\" weights ของ backbone ไม่ให้เปลี่ยนแปลง\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # เปลี่ยน layer สุดท้ายของโมเดลที่เลือกใช้งาน\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "```\n",
    "\n",
    "\n",
    "**โหลดโมเดล ResNet18 จาก torchvision.models และส่งเข้า class TransferNet ของเรา**\n",
    "``` py\n",
    "backbone = torchvision.models.resnet18(pretrained=True)\n",
    "net = TransferNet(backbone = backbone, num_classes = 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ทำการเทรนด้วย `train_loop()` และ `test_loop()` ที่ได้เขียนไว้**\n",
    "\n",
    "``` py\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "epochs = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # loss function for classification\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate) # Optimizer\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"min\")\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, net, loss_fn, optimizer)\n",
    "    val_loss = test_loop(val_dataloader, net, loss_fn, optimizer)\n",
    "    scheduler.step(val_loss)\n",
    "print(\"Done!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVScMb8e5qXG"
   },
   "source": [
    "## **Transfer Learning ด้วย Pytorch Lightning**\n",
    "\n",
    "นอกจากการเขียน Transfer learning ด้วย Pytorch นั้น เรายังสามารถใช้ไลบรารี่อื่นๆที่ทำให้การเทรนโมเดลของเราสั้นลงได้ เช่น Pytorch Lightning โดยวิธีการเขียนสามารถดูได้ตามด้านล่าง"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install ไลบรารี่ pytorch_lightning**\n",
    "``` py\n",
    "!pip install pytorch_lightning\n",
    "```\n",
    "\n",
    "\n",
    "**Define โมเดลของเราโดย inherit จาก pl.LightningModule แทน nn.Module**\n",
    "``` py\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransferNet(pl.LightningModule): \n",
    "    def __init__(self, backbone, num_classes, learning_rate):\n",
    "        super(TransferNet, self).__init__()\n",
    "        self.model = backbone\n",
    "        self.learning_rate = learning_rate\n",
    "        # \"freeze\" weights ของ backbone ไม่ให้เปลี่ยนแปลง\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # เปลี่ยน layer สุดท้ายของโมเดลที่เลือกใช้งาน\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    # กำหนด loop การเทรนโมเดลภายใน class\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        prediction = self(x)\n",
    "        loss = F.cross_entropy(prediction, y)\n",
    "        return loss\n",
    "\n",
    "    # กำหนดเลือก optimizer ที่นี่เช่นเดียวกัน\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **เทรนด้วย `pl.Trainer()`**\n",
    "\n",
    "``` py\n",
    "trainer = pl.Trainer(max_epochs=50, accelerator = \"gpu\")\n",
    "backbone = torchvision.models.resnet18(pretrained=True)\n",
    "net = TransferNet(\n",
    "    backbone = backbone,\n",
    "    num_classes = 2,\n",
    "    learning_rate=1e-3) \n",
    "\n",
    "trainer.fit(net, train_dataloaders=train_dataloader)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUZ1i6Ft5XIu"
   },
   "source": [
    "## **Transfer Learning ด้วย FastAI**\n",
    "\n",
    "``` py\n",
    "# https://www.analyticsvidhya.com/blog/2021/05/training-state-of-the-art-deep-learning-models-with-fast-ai/\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "\n",
    "# กำหนด folder ที่เก็บข้อมูลไว้ให้กับ ImageDataLoaders\n",
    "# ภายใต้ path ที่กำหนดจะมี 2 folders ย่อย คือ training/ และ validation/ \n",
    "dls = ImageDataLoaders.from_folder(path=path, \n",
    "                                    train='training',\n",
    "                                    valid='validation',\n",
    "                                    valid_pct=0.2,\n",
    "                                    shuffle=True)\n",
    "\n",
    "# กำหนด backbone ที่เราต้องการใช้งานให้กับ cnn_learner เช่น resnet18\n",
    "learner = cnn_learner(dls, \n",
    "                    resnet18, \n",
    "                    metrics=[accuracy, error_rate])\n",
    "\n",
    "learner.fine_tune(4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Augmentations ด้วย `torchvision.transforms`**\n",
    "\n",
    "``` py\n",
    "import torchvision.transforms as T\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.TrivialAugmentWide(),\n",
    "    T.RandomResizedCrop((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225),)\n",
    "])\n",
    "\n",
    "train_data = AnimalDataset(train_filenames, train_labels, transform=train_transform)\n",
    "val_data = AnimalDataset(val_filenames, val_labels, transform=val_transform)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDjHIZlekf8J"
   },
   "source": [
    "# **มาทดลองการจำแนกรูปภาพกัน**\n",
    "\n",
    "Taken from [AI builders' repository](https://github.com/ai-builders/curriculum/blob/main/notebooks/04v_classification_pytorch.ipynb)\n",
    "\n",
    "**Authored by**: Titipat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWMpWoXwP-PO"
   },
   "outputs": [],
   "source": [
    "## upload your kaggle.json file to colab workspace first\n",
    "## before running this cell\n",
    "\n",
    "!mkdir /root/.kaggle\n",
    "!cp kaggle.json /root/.kaggle\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jUkEPDBPQqi0"
   },
   "outputs": [],
   "source": [
    "# SOURCE: https://www.kaggle.com/competitions/dog-breed-identification/data\n",
    "\n",
    "!kaggle competitions download -c dog-breed-identification\n",
    "!unzip dog-breed-identification.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOW68WLk52Nb"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning\n",
    "!pip install --upgrade numpy # จำเป็นเพื่อป้องกัน errors จากจุดทศนิยม"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPE1UlYk6L5J"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import shutil\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kd067cYu6L5K"
   },
   "outputs": [],
   "source": [
    "# get all image paths\n",
    "img_df = pd.DataFrame(glob(\"data/train/*.jpg\"), columns=[\"path\"])\n",
    "img_df[\"id\"] = img_df.path.map(lambda x: op.basename(x).replace(\".jpg\", \"\"))\n",
    "\n",
    "# read label data\n",
    "label_df = pd.read_csv(\"data/labels.csv\")\n",
    "train_df = img_df.merge(label_df, on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "LETRAbRA6L5L",
    "outputId": "3faaace2-c1dc-4825-bfb4-fb221dda6e83"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OW6ePIk6L5L"
   },
   "outputs": [],
   "source": [
    "train_df, validation_df = train_test_split(train_df, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEPFUry56L5L",
    "outputId": "ffd261bb-49b7-4fb6-8fe2-4b3f2e1364f7"
   },
   "outputs": [],
   "source": [
    "print(\"Length of training set = {}, validation set = {}\".format(len(train_df), len(validation_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDzvopLj6L5M"
   },
   "outputs": [],
   "source": [
    "root_dir = \"data/dogdata/\"\n",
    "for df, f in zip([train_df, validation_df], [\"train\", \"validation\"]):\n",
    "    for _, r in df.iterrows():\n",
    "        # create subfolder if it doesn't exist\n",
    "        d = op.join(root_dir, f, r.breed)\n",
    "        if not op.exists(d):\n",
    "            os.makedirs(d)\n",
    "        shutil.copy(r.path, op.join(root_dir, f, r.breed, f\"{r.id}.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfwDAW3c6L5R"
   },
   "source": [
    "### **Image classification ด้วย Pytorch Lightning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sl82Yb2p6L5R"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import Accuracy\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsTeBEG-6L5R"
   },
   "outputs": [],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.TrivialAugmentWide(),\n",
    "    T.RandomResizedCrop((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225),)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELA3zUpT6L5R"
   },
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(\"data/dogdata/train/\", transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(\"data/dogdata/validation/\", transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDEYrrjv6L5R"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D--CH1M56L5R"
   },
   "outputs": [],
   "source": [
    "classes = train_data.classes\n",
    "n_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4W4pNnV86L5R"
   },
   "outputs": [],
   "source": [
    "class DogResNet(pl.LightningModule):\n",
    "    def __init__(self, n_classes=120):\n",
    "        super(DogResNet, self).__init__()\n",
    "        \n",
    "        # จำนวนของพันธุ์น้องหมา (120)\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # ใช้สถาปัตยกรรม resnet34; เปลี่ยน layer สุดท้าย\n",
    "        self.backbone = models.resnet34(pretrained=True)\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        # เปลี่ยน fc layer เป็น output ขนาด 120\n",
    "        self.backbone.fc = torch.nn.Linear(self.backbone.fc.in_features, n_classes)\n",
    "        \n",
    "        self.entropy_loss = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy()\n",
    "\n",
    "    def forward(self, x):\n",
    "        preds = self.backbone(x)\n",
    "        return preds\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.backbone(x)\n",
    "        loss = self.entropy_loss(logits, y)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        \n",
    "        # log metrics ที่สำคัญไว้เพื่อการวิเคราะห์ในภายหลัง\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", acc)\n",
    "        return {\"loss\": loss, \"accuracy\": acc}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.backbone(x)\n",
    "        loss = self.entropy_loss(logits, y)\n",
    "        y_pred = torch.argmax(logits, dim=1)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        \n",
    "        # log metrics ที่สำคัญไว้เพื่อการวิเคราะห์ในภายหลัง\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        return {\"loss\": loss, \"accuracy\": acc}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.backbone(x)\n",
    "        loss = self.entropy_loss(logits, y)\n",
    "        y_pred = torch.argmax(logits, dim=1)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        return {\"loss\": loss, \"accuracy\": acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        return {\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160,
     "referenced_widgets": [
      "1a9ad9ac419940cca811f5b800379951",
      "5595b6ad7fc844afb015f3bed026cde4",
      "ee91d4c17b384eb8b2696ca77de6e3f9",
      "8e01aca8215941adb20a3bb5c5ef94eb",
      "b801fac7f8ce43d3983205bd11a95b39",
      "0f5c025d1cc9494ba56feaeb4f4419c6",
      "401f3bf4c9fa4f67a7ddd2d1d0b6c181",
      "48aac8b54ccf48118b132fad5dc6f618",
      "2bb7ea5700a14b97ac2a39639f4d905f",
      "3394f607556f4132a19e6ccf2529e134",
      "54e3ec41cfe941d9bd9709f165adde4a"
     ]
    },
    "id": "DNhox75t6L5S",
    "outputId": "7897d592-6f4f-4c9b-c5d1-1ba73b6dd669"
   },
   "outputs": [],
   "source": [
    "model = DogResNet(n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jB9_eBHl6L5S"
   },
   "outputs": [],
   "source": [
    "# callback เพื่อให้ Trainer เซฟโมเดลไว้ในไฟล์ checkpoint เมื่อผลลัพธ์ val_loss ลดต่ำกว่าที่เคย\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"./checkpoints/dogbreed/\",\n",
    "    filename=\"resnet18--{epoch:02d}-{val_acc:.2f}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616,
     "referenced_widgets": [
      "6ef85ce5690e4b8a80eb2238cca2279d",
      "feb42ccc232f41778d76e333f2abc25f",
      "5559fc499c4640f4909ed4eb5808ba12",
      "92b632a7338f4b52a128fd5b27814828",
      "24dba9b8e75347e7a6595aca7e051207",
      "2d0ada5928ae4cd4bd3b49a18f4d9cda",
      "4e9a9b836e244419b28df7c2c1e75dd6",
      "b3ebde3c574c477aa97d48246663d585",
      "015a982867e24a6fb1b2d613ab3e51d7",
      "d17b7d61e718474984e7671dea4a108a",
      "8c1ac2ffb2274d18bbd515cfca8cfc74",
      "afca2bdebe6949dca964012a58bdefc7",
      "f27e9cf8c0f74e888eb060f011f916b8",
      "c3d8adbe49ed4557b1b93ba1df4783d0",
      "b268abc60662486aa3db43783acdcd96",
      "e186bf625db2498ab47cb2b3e8e7c097",
      "d39f89bb90e54598823a279ca6b40198",
      "4eb29472f21e4da7bf08970986214fe9",
      "fc7db8a85daa4b17917e7f1d4f9535d4",
      "529e438c10cc4b3395ed1646e92aafe5",
      "6187c0d3a56a47d9a07578244de6bb96",
      "dd74400a78f74e38aba0c49d4bab6420",
      "04042eca3fe44ad7ac9d813ca77049b2",
      "6e88793ce00c4d4e97a4f70aeded40a6",
      "3ca8b48f6ac746aeb385de7bf2113706",
      "598f96eeda2449b2b5778012ff09c031",
      "7ed8f29e74c14445a03e82c1437f14eb",
      "c4672705c9c44b899e0a9076586af86a",
      "84255454821349028d421c0592138b85",
      "9f02bcb358694b65be4e6764c199146e",
      "70c9a5a3a4df45859dee0c1008ac96d5",
      "1dc4dd7a53f24949ad52e9526df976ad",
      "4a52f9b29adc4fa3821b42298c45b6d8",
      "a7ac2b42598e40cbb92dd2dcb26833e5",
      "ec57a6335a874e05b1655b459f0e0cab",
      "80b1cc7b1bf944b2accfbc81de676bae",
      "74c9a0f55029436d9bd34283d09e52ea",
      "675a7c1046274ea8b8f8961c2319ad4c",
      "762cb753770a4911b0fc8701f5e37d98",
      "bcec093377684f1aae7916ff6c85607b",
      "bdf095d4fee540c2a084d0b332276148",
      "4d434af0fb6245038f69b14c49464bf1",
      "105913e82aa442458de0694e512451b7",
      "521da0858c654f9cb052cda1d27819ac",
      "f99c09dd785541b6b69d6a037cb0ff99",
      "55f9d9a5aae642b99d2b0fd0ab7b73b5",
      "1b0417c1d19a41d28a6c7986b14def6b",
      "822a1f76326d43638d871cfea5099351",
      "dafee7b8c65a4c53b92a02e30ec2becd",
      "8299165b90034000b3b3ae7ef14dd7a5",
      "b5b17a4f06c34bd69b3b8fcd9ecd6bbf",
      "2e033704e2884d57b3c2cedda9f6036c",
      "5e3cc4fe7ddb4299b4b8048658f5c88a",
      "ad7b3ee6681a445bafb243d13e6efb85",
      "7174c253ef7c489cb9c40de15b96565a",
      "c919c2c368bc4451bd6eae110d0b286b",
      "1023d60846a446ccb64392a607c2bdb8",
      "a8764a8ac5eb4f67ba70339e3951f7e3",
      "0f4ee9254d114e88b919fcbb6985a4b2",
      "d82e58f8eed749a89aa76f37e06589ef",
      "87ad51174dee47148cf1dae430e35366",
      "9cbaecc0af1b45dd9315f21e50dfbc80",
      "3b3d43b6505f4f06aaff196ffaf2ede1",
      "c34c99465a014662a754efbbdb878d40",
      "f7d49c88419f4869b85ede21f47c3fdc",
      "02ed42e436d94d8a908826b747d38e98",
      "6b82f62b48704a4a8717efd4affed643",
      "ff044c7c67ac4d5eb06823d0de1ebceb",
      "be51f020caf948b7a1ea0964ad572f54",
      "d85b861923e541ec80486782f16e090e",
      "8e85392f24374c41bf2380dd54c2cf23",
      "2041473eb52e49adbfe95b08241d359d",
      "43e09db6a4a643c2a0330d2295f59387",
      "07a927c844f84103a67a68bd0afd8371",
      "df6da470047f47a0b6d15bf95c142ebd",
      "fc9c1fd2412a4339bb983d503325945c",
      "942ccc10aea24e969148fb14b2649c03",
      "855d8f1717b544088b8d501d0e64a327",
      "f2f0a893e2514e208dc2e68a34bdccc4",
      "f9e6e8fb4f284f019624b5e0c2ae9217",
      "7180216217c64b7691b75b89ac460bc8",
      "2e979c55e5b141d398206f9147192d9a",
      "e0a2753d463f47d1bb90ecb16dda98c6",
      "065aa6a48d93465db1b00477a02b9606",
      "a283163f6d0e42f1a4502cda7dbe714a",
      "5b8a24dd176c49bb841bbbf083a6572d",
      "707438cda25f4c6084a3960492e4a315",
      "e76efc29a84a45db8dae40034836af7c",
      "2a205a4f4cb74eca8b147f1bb63b494a",
      "ec1d76f7705d483c8a7f88395ee72c22",
      "fe9353bcfa7c4ff6ac1dfa71cd6f2e90",
      "eef00af08b344ffa887a4036145c5e6f",
      "2cadeac6267245a982732d73714b8475",
      "f86d43b75a7c4297b6b2dc2c10959796",
      "097b48dff22d43cea28ec2a909c6b4f8",
      "e0d9518daa154206b1348747a3a20a70",
      "069d690dd944461597a23a6e8c2ec545",
      "ee19debd93ee4afdb8d65ac50cb84e10",
      "f6623e587c6e4b64a802b16b7e042602",
      "0cf82c179b5c4895b7d72ad24bb4a3e4",
      "18b93da064c04c319dcf9ec9895309fd",
      "a010c43edcbc489e80e6cd474072e43b",
      "3a8f6729b64e4b29aa8e6da4a572d3d9",
      "36985c8856d64c75b3a45b24ef454e1a",
      "c57e583e8fe44e448d9e92d206e4ab9b",
      "518ce2ec6b0742c48fbf484d5c477695",
      "3456b40ddd0d474291f92822c29ea63c",
      "5fef662579404ff882529a51682b808b",
      "5a3f406a15ce49cdaa093d5564ca0404",
      "436123c59a0b4b91a5bdc9b50692365c",
      "797ecb3ffd5f40b08c24f51864d796d4",
      "787f320aeb534fb3845b6ab2bb2516ed",
      "368a39794c9c44e7ba069a1981281b01",
      "a6324c469d544585a440ee74b5c0e511",
      "a91a44cbb464464f9034eb1388543831",
      "4c528c9f0ea14307b90351c54bde2a17",
      "74465cf5c1844c7aa5244b63b46c1c42",
      "1c21a062618945a89af8e8db9fc3172d",
      "8ff462b2cada4d4fb0591429156c79e0",
      "68a0b7f805794a1199d309b332eff50e",
      "35ac5fc0384a4a54ba72a13565ba4fe2",
      "c3e4be5bfd9b44979a74c78bdfc953c9",
      "082fd398d3de40bc93159a3222d36e46",
      "8c5a222b47394c4784f15847e0670562",
      "2cfee86f33b44575ba700b45308c2e0b",
      "04ebb3de4d15465c9322ea9538503aa0",
      "1333613472014aaba316e0ced52d5c6f",
      "bdf7574be9ec45d1a8725ca8663a7d74",
      "414a1913eac0470c84889fb1df0643f7",
      "d3861119bbc349689b68a64ff31d921f",
      "d483ab98f32344f39738f03d7ed5d2ab",
      "46955b1a2fc14cc3af8d106983ee2625"
     ]
    },
    "id": "D29xYXLN6L5S",
    "outputId": "5cec40b1-5b4c-4df7-c95e-bae1966c20fa"
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=10, accelerator=\"gpu\", callbacks=[checkpoint_callback])\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Image Classification ด้วย FastAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fastai --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2021/05/training-state-of-the-art-deep-learning-models-with-fast-ai/\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision.all import *\n",
    "\n",
    "# กำหนด folder ที่เก็บข้อมูลไว้ให้กับ ImageDataLoaders\n",
    "# ภายใต้ path ที่กำหนดจะมี 2 folders ย่อย คือ training/ และ validation/ \n",
    "dls = ImageDataLoaders.from_folder(path=\"data/dogdata\", \n",
    "                                    train=\"train\",\n",
    "                                    valid=\"validation\",\n",
    "                                    item_tfms=Resize(224),\n",
    "                                    bs = 64,\n",
    "                                    shuffle=True)\n",
    "\n",
    "# กำหนด backbone ที่เราต้องการใช้งานให้กับ cnn_learner เช่น resnet18\n",
    "learner = cnn_learner(dls,\n",
    "                    resnet18,\n",
    "                    metrics=[accuracy, error_rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fine_tune(10, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **แบบทดสอบ**\n",
    "\n",
    "1. จงสร้าง Convolutional layer ที่รับ input\n",
    "``` py\n",
    "dummy_input = torch.rand(3, 28, 28)\n",
    "```\n",
    "โดยมี filters จำนวน 64 filters, `kernel_size = 3` และ มี padding\n",
    "\n",
    "2. จงคำนวณขนาด (dimensions) ของ output เมื่อผ่าน convolutional layer ดังกล่าว\n",
    "\n",
    "3. จงสร้าง Max pooling layer ที่รับ output ของ convolutional layer ในข้อก่อนหน้าเป็น input โดยมีขนาด `kernel_size = 2` และบอกขนาด (dimensions) ของ output เมื่อผ่าน Max pooling layer ดังกล่าว"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a2a344cc8269451be292df9fb07df2607f71d7304dd683c993c608f0927a913c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
